<meta charset="utf-8"/>
<co-content>
 <p>
  Welcome to the forth week of the "How to Win a Data Science Competition" course. Here is a short summary of what you will learn.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <em>
     Dmitry Ulyanov
    </em>
    will start this week with
    <strong>
     hyperparameter optimization
    </strong>
    . We will understand hyperparameter tuning process in general, list most important hyperparameters for major machine learning models and describe their impact. We will also have a special video with
    <strong>
     practical tips and tricks
    </strong>
    , recorded by four instructors.
   </p>
  </li>
 </ul>
 <ul bullettype="bullets">
  <li>
   <p>
    <em>
     Mikhail Trofimov
    </em>
    and
    <em>
     Dmitry Altukhov
    </em>
    will discuss
    <strong>
     advanced features
    </strong>
    that we can extract from the data. We will describe matrix factorization technique for feature extraction, learn to create features based on t-SNE, review concept of feature interactions, and learn to make up new features based on statistics and nearest neighbors. If you feel yourself strong enough we recommend you to take a look at
    <u>
     Honors track programming assignment
    </u>
    , where you will need to implement features based on nearest neighbors, which are often could provide an edge in a competition.
   </p>
  </li>
 </ul>
 <ul bullettype="bullets">
  <li>
   <p>
    <em>
     Marios Michailidis
    </em>
    , kaggle top-1,
    <em>
    </em>
    will review
    <strong>
     ensembling methods
    </strong>
    . We will describe and compare ensembling methods such as weighted averaging, bagging, boosting, stacking. We will outline plan of validation schemes, discuss practical tips and tricks, and implement ensembling in the
    <u>
     programming assignment
    </u>
    .
   </p>
  </li>
 </ul>
 <p>
  To keep up the work on the
  <u>
   final project
  </u>
  , apply ideas we will discuss and improve your current solution by optimizing your hyperparameters, maybe implementing some of the advanced features and for sure trying out ensembling.
 </p>
 <p>
  Now let's go ahead and get started!
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
