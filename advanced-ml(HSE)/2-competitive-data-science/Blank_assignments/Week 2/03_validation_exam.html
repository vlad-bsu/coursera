<meta charset="utf-8"/>
<h3>
 Question 1
</h3>
<co-content>
 <p>
  Select true statements
 </p>
</co-content>
<form>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    We use validation to estimate the quality of our model
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    Underfitting refers to not capturing enough patterns in the data
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    The logic behind validation split should mimic the logic behind train-test split.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    The model, that performs best on the validation set is guaranteed to be the best on the test set.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="checkbox"/>
  <co-content>
   <span>
    Performance increase on a fixed cross-validation split guaranties performance increase on any cross-validation split.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 2
</h3>
<co-content>
 <p>
  Usually on Kaggle it is allowed  to select two final submissions, which will be checked against the private LB and contribute to the competitor's final position. A common practice is to select one submission with a best validation score, and another submission which scored best on Public LB. What is the logic behind this choice?
 </p>
</co-content>
<form>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    Generally, this approach is based on the assumption that the test data may have a different target distribution compared to the train data. If that would be the true, the submission which was chosen based on Public LB, will perform better. If, otherwise, the above distributions will be similar, the submission which was chosen based on validation scores, will perform better.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    Generally, this approach is based on the assumption that people rarely tend to overfit to the Public LB. Almost always you have a lot of data in the test set and it is quite hard to overfit. Indeed, this render validation useless.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    Generally, this approach is based on the assumption that validation is rarely valid in competitions. Often it is hard to trust your validation and thus you should account for both cases if the validation will succeed and if the validation will fail.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 3
</h3>
<co-content>
 <p>
  Suppose we have a competition where we are given a dataset of marketing campaigns. Each campaign runs for a few weeks and for each day in campaign we have a target - number of new customers involved. Thus the row in a dataset looks like
 </p>
 <p>
  Campaign_id,  Date, {some features}, Number_of_new_customers
 </p>
 <p>
  Test set consists of multiple campaigns. For each of them we are given several first days in train data. For example, if a campaign runs for two weeks, we could have three first days in train set, and all next days will be present in the test set.
 </p>
 <p>
  Identify train/test split in a competition.
 </p>
</co-content>
<form>
 <label>
  <input name="2" type="radio"/>
  <co-content>
   <span>
    Random split
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="radio"/>
  <co-content>
   <span>
    Time-based split
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="radio"/>
  <co-content>
   <span>
    Id-based split
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="radio"/>
  <co-content>
   <span>
    Combined split
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 4
</h3>
<co-content>
 <p>
  Which of the following problems you usually can identify without the Leaderboard?
 </p>
</co-content>
<form>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Public leaderboard score will be unreliable because of too little data
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Different scores/optimal parameters between folds
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Train and test target distribution are from different distributions
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Train and test data are from different distributions
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
