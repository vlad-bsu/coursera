<meta charset="utf-8"/>
<h3>
 Question 1
</h3>
<co-content>
 <p>
  If you have 10,000,000 examples, how would you split the train/dev/test set?
 </p>
 <p>
 </p>
</co-content>
<form>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    60% train . 20% dev . 20% test
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    98% train . 1% dev . 1% test
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="0" type="radio"/>
  <co-content>
   <span>
    33% train .  33% dev . 33% test
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 2
</h3>
<co-content>
 <p>
  The dev and test set should:
 </p>
 <p>
 </p>
 <p>
 </p>
</co-content>
<form>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    Come from the same distribution
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    Come from different distributions
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    Be identical to each other (same (x,y) pairs)
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="1" type="radio"/>
  <co-content>
   <span>
    <strong>
     Have the same number of examples
    </strong>
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 3
</h3>
<co-content>
 <p>
  If your Neural Network model seems to have high variance, what of the following would be promising things to try?
 </p>
</co-content>
<form>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Get more test data
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Increase the number of units in each hidden layer
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Get more training data
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Make the Neural Network deeper
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="2" type="checkbox"/>
  <co-content>
   <span>
    Add regularization
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 4
</h3>
<co-content>
 <p>
  You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)
 </p>
</co-content>
<form>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Increase the regularization parameter lambda
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Decrease the regularization parameter lambda
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Get more training data
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="3" type="checkbox"/>
  <co-content>
   <span>
    Use a bigger neural network
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 5
</h3>
<co-content>
 <p>
  What is weight decay?
 </p>
</co-content>
<form>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    Gradual corruption of the weights in the neural network if it is trained on noisy data.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    The process of gradually decreasing the learning rate during training.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="4" type="radio"/>
  <co-content>
   <span>
    A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 6
</h3>
<co-content>
 <p>
  What happens when you increase the regularization hyperparameter lambda?
 </p>
</co-content>
<form>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Weights are pushed toward becoming smaller (closer to 0)
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Weights are pushed toward becoming bigger (further from 0)
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Doubling lambda should roughly result in doubling the weights
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="5" type="radio"/>
  <co-content>
   <span>
    Gradient descent taking bigger steps with each iteration (proportional to lambda)
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 7
</h3>
<co-content>
 <p>
  With the inverted dropout technique, at test time:
 </p>
</co-content>
<form>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span>
    You apply dropout (randomly eliminating units) and do not keep the 1/keep_prob factor in the calculations used in training
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span>
    You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span>
    You apply dropout (randomly eliminating units) but keep the 1/keep_prob factor in the calculations used in training.
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="6" type="radio"/>
  <co-content>
   <span>
    You do not apply dropout (do not randomly eliminate units), but keep the 1/keep_prob factor in the calculations used in training.
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 8
</h3>
<co-content>
 <p>
  Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply)
 </p>
</co-content>
<form>
 <label>
  <input name="7" type="checkbox"/>
  <co-content>
   <span>
    Increasing the regularization effect
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="7" type="checkbox"/>
  <co-content>
   <span>
    Reducing the regularization effect
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="7" type="checkbox"/>
  <co-content>
   <span>
    Causing the neural network to end up with a higher training set error
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="7" type="checkbox"/>
  <co-content>
   <span>
    Causing the neural network to end up with a lower training set error
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 9
</h3>
<co-content>
 <p>
  Which of these techniques are useful for reducing variance (reducing overfitting)? (Check all that apply.)
 </p>
</co-content>
<form>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    Xavier initialization
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    Vanishing gradient
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    Data augmentation
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    L2 regularization
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    Exploding gradient
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    Dropout
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="8" type="checkbox"/>
  <co-content>
   <span>
    Gradient Checking
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<h3>
 Question 10
</h3>
<co-content>
 <p hasmath="true">
  Why do we normalize the inputs $$x$$?
 </p>
</co-content>
<form>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span>
    It makes the cost function faster to optimize
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span>
    Normalization is another word for regularization--It helps to reduce variance
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span>
    It makes it easier to visualize the data
   </span>
  </co-content>
  <br/>
 </label>
 <label>
  <input name="9" type="radio"/>
  <co-content>
   <span>
    It makes the parameter initialization faster
   </span>
  </co-content>
  <br/>
 </label>
</form>
<hr/>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
